import cv2
import numpy as np
import os
import mediapipe as mp
import random
from tqdm import tqdm

# Initialisation Mediapipe
mp_holistic = mp.solutions.holistic
mp_face_mesh = mp.solutions.face_mesh
mp_drawing = mp.solutions.drawing_utils

# Configuration des modèles
holistic = mp_holistic.Holistic(
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5,
    model_complexity=2  # Modèle complet avec visage
)

# Paramètres d'augmentation
OCCLUSION_PROBABILITY = 0.3  # Probabilité d'occlusion d'un landmark
FACE_DETECTION_CONFIDENCE = 0.5  # Seuil de détection du visage

def extract_keypoints(results, include_face=True):
    """Extrait les keypoints avec gestion des données manquantes"""
    # Points de pose (33 points)
    pose = np.array([[res.x, res.y, res.z, res.visibility] 
                    for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)
    
    # Main gauche (21 points)
    lh = np.array([[res.x, res.y, res.z] 
                  for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    
    # Main droite (21 points)
    rh = np.array([[res.x, res.y, res.z] 
                  for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    
    # Points faciaux (478 points avec iris)
    face = np.array([[res.x, res.y, res.z] 
                    for res in results.face_landmarks.landmark]).flatten() if (include_face and results.face_landmarks) else np.zeros(478*3)
    
    return np.concatenate([pose, lh, rh, face])

def apply_random_occlusion(keypoints):
    """Applique une occlusion aléatoire à certains landmarks"""
    occluded_keypoints = keypoints.copy()
    
    # Structure des keypoints: [pose(33*4), lh(21*3), rh(21*3), face(478*3)]
    section_ranges = [
        (0, 132),          # Pose (33 points * 4 valeurs)
        (132, 132+63),     # Main gauche (21*3)
        (132+63, 132+126), # Main droite (21*3)
        (132+126, 132+126+1434) # Visage (478*3)
    ]
    
    # Probabilités d'occlusion différentes pour chaque section
    occlusion_probs = [0.3, 0.4, 0.4, 0.2]  # Pose, mains, visage
    
    for idx, (start, end) in enumerate(section_ranges):
        section_length = end - start
        elements_per_point = 4 if idx == 0 else 3  # Pose a 4 valeurs, les autres 3
        
        # Nombre de points dans cette section
        num_points = section_length // elements_per_point
        
        # Sélection aléatoire des points à occulter
        num_to_occlude = int(num_points * occlusion_probs[idx] * random.uniform(0.7, 1.3))
        points_to_occlude = random.sample(range(num_points), num_to_occlude)
        
        # Application de l'occlusion
        for point_idx in points_to_occlude:
            start_idx = start + point_idx * elements_per_point
            end_idx = start_idx + elements_per_point
            occluded_keypoints[start_idx:end_idx] = 0
    
    return occluded_keypoints

def mediapipe_detection(image, model, include_face=True):
    """Détection Mediapipe avec gestion optionnelle du visage"""
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image.flags.writeable = False
    results = model.process(image)
    image.flags.writeable = True
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    return image, results

def apply_transformations(frame):
    """Applique une séquence de transformations à l'image"""
    # Liste des transformations possibles
    transformations = [
        lambda img: cv2.flip(img, random.choice([-1, 0, 1])),  # Flip aléatoire
        lambda img: cv2.rotate(img, random.choice([
            cv2.ROTATE_90_CLOCKWISE,
            cv2.ROTATE_180,
            cv2.ROTATE_90_COUNTERCLOCKWISE
        ])),
        lambda img: cv2.GaussianBlur(img, (random.choice([3,5,7]), 0),
        lambda img: cv2.convertScaleAbs(img, alpha=random.uniform(0.7, 1.3), beta=random.uniform(-30, 30)),
        lambda img: cv2.cvtColor(img, cv2.COLOR_BGR2HSV),  # Changement d'espace colorimétrique
        lambda img: cv2.cvtColor(img, cv2.COLOR_HSV2BGR)
    ]
    
    # Appliquer 2-4 transformations aléatoires
    transformed = frame.copy()
    num_transforms = random.randint(2, 4)
    for _ in range(num_transforms):
        transform = random.choice(transformations)
        transformed = transform(transformed)
    
    return transformed

def main():
    DATA_PATH = os.path.join('MP_DataAugmented_Occluded')
    VIDEO_PATH = r'E:\Dataset1'
    sequence_length = 30
    actions = [d for d in os.listdir(VIDEO_PATH) if os.path.isdir(os.path.join(VIDEO_PATH, d))]
    
    with mp_holistic.Holistic(
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5,
        model_complexity=2
    ) as holistic:
        
        for action in tqdm(actions, desc="Processing actions"):
            action_path = os.path.join(VIDEO_PATH, action)
            video_files = [f for f in os.listdir(action_path) if f.endswith(('.mp4', '.avi', '.mov'))]
            
            for video_idx, video_file in enumerate(tqdm(video_files, desc=f"Processing {action}")):
                video_path = os.path.join(action_path, video_file)
                cap = cv2.VideoCapture(video_path)
                
                # Dossiers de sauvegarde
                complete_path = os.path.join(DATA_PATH, action, str(video_idx), 'complete')
                occluded_path = os.path.join(DATA_PATH, action, str(video_idx), 'occluded')
                os.makedirs(complete_path, exist_ok=True)
                os.makedirs(occluded_path, exist_ok=True)
                
                frame_num = 0
                while cap.isOpened() and frame_num < sequence_length:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    
                    # Appliquer les transformations d'image
                    transformed_frame = apply_transformations(frame)
                    
                    # Détection Mediapipe
                    _, results = mediapipe_detection(transformed_frame, holistic, include_face=True)
                    
                    # Extraire les keypoints complets (avec visage)
                    complete_keypoints = extract_keypoints(results, include_face=True)
                    
                    # Générer une version avec occlusion aléatoire
                    occluded_keypoints = apply_random_occlusion(complete_keypoints)
                    
                    # Sauvegarder les deux versions
                    np.save(os.path.join(complete_path, str(frame_num)), complete_keypoints)
                    np.save(os.path.join(occluded_path, str(frame_num)), occluded_keypoints)
                    
                    # Visualisation (optionnelle)
                    vis_frame = transformed_frame.copy()
                    
                    # Dessiner les landmarks visibles seulement
                    if results.pose_landmarks:
                        mp_drawing.draw_landmarks(
                            vis_frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
                    
                    if results.left_hand_landmarks:
                        mp_drawing.draw_landmarks(
                            vis_frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
                    
                    if results.right_hand_landmarks:
                        mp_drawing.draw_landmarks(
                            vis_frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
                    
                    if results.face_landmarks:
                        mp_drawing.draw_landmarks(
                            vis_frame, results.face_landmarks, mp_face_mesh.FACEMESH_TESSELATION)
                    
                    # Informations
                    cv2.putText(vis_frame, f'{action} - Video {video_idx}', (10, 30),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    cv2.putText(vis_frame, f'Frame {frame_num}/{sequence_length}', (10, 60),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    cv2.putText(vis_frame, 'Occluded Landmarks Training', (10, 90),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    
                    cv2.imshow('Data Collection - Occlusion Training', vis_frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break
                    
                    frame_num += 1
                
                cap.release()
    
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
